# @package _global_

# pl trainer params
trainer:
  logger: False
  checkpoint_callback: True
  default_root_dir: null
  profiler: null
  tpu_cores: null
  fast_dev_run: False
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0

  resume_from_checkpoint: null
  benchmark: true
  reload_dataloaders_every_epoch: False
  detect_anomaly: false
  automatic_optimization: null
  deterministic: False
  num_sanity_val_steps: 0
  replace_sampler_ddp: true
  accumulate_grad_batches: 1

  sync_batchnorm: true
  auto_scale_batch_size: null
  strategy: null
  precision: 32

  # progress_bar_refresh_rate: 5
  log_every_n_steps: 10
  check_val_every_n_epoch: 1
  weights_save_path: outputs/ckpt/${model_name}
  max_epochs: 100
  gpus: ${gpus}
  num_nodes: ${nodes}
